{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize, FreqDist,regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer,PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans, AffinityPropagation\n",
    "from sklearn.cluster import DBSCAN, OPTICS,MeanShift\n",
    "from scipy.cluster.hierarchy import dendrogram,linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from itertools import takewhile\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Code/Functionality.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Universe.SIC.apply(lambda s:str(s)[:1]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock Price Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes for Correlations of Daily Changes (Absolute & Relative to Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_prices,abs_data,rel_data=collect_market_data(Universe.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Look @ Correlations Between Daily Changes & Daily Changes relative to the S&P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look @ Some Heatmaps of Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(nrows=1,ncols=1,figsize=(25,15))\n",
    "ax.set_title('Correlations btwn 2011 & 2020')\n",
    "mtrx=abs_data[1]\n",
    "mask = np.zeros_like(mtrx)\n",
    "mask[np.triu_indices_from(mtrx)] = True\n",
    "sns.heatmap(mtrx,vmin=-1,vmax=1,cmap='RdBu',ax=ax,mask=mask);\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "fig.savefig(image_path+'Corr Matrix.pdf',\n",
    "            orientation='landscape',\n",
    "            pad_inches=0.0,bbox_inches='tight',\n",
    "            format='pdf'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(nrows=1,ncols=1,figsize=(25,15))\n",
    "ax.set_title('Relative Correlations btwn 2011 & 2020')\n",
    "mtrx=rel_data[1]\n",
    "mask = np.zeros_like(mtrx)\n",
    "mask[np.triu_indices_from(mtrx)] = True\n",
    "sns.heatmap(mtrx,vmin=-1,vmax=1,cmap='RdBu',ax=ax,mask=mask);\n",
    "fig.savefig(image_path+'Corr Matrix_Relative.pdf',\n",
    "            orientation='landscape',pad_inches=0.0,bbox_inches='tight',\n",
    "            format='pdf'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=5,nrows=2,figsize=(25,15),sharex=True,sharey=True,)\n",
    "fig.suptitle('Rltve. Corr. by Year (2011-2020)')\n",
    "axes=ax.reshape(-1)\n",
    "i=0\n",
    "for y in coverage:\n",
    "    a=axes[i]\n",
    "    a.set_title(y)\n",
    "    corr_mat=rel_data[2].xs(y+'-12-31')\n",
    "    mask = np.zeros_like(corr_mat)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(corr_mat,mask=mask,ax=a,vmin=-1,vmax=1,cbar=False,cmap='RdBu')\n",
    "    i+=1\n",
    "fig.savefig(image_path+'Annual Corr Matrices_Relative.pdf',\n",
    "            orientation='landscape',\n",
    "            pad_inches=0.0,\n",
    "            bbox_inches='tight',\n",
    "            format='pdf'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the number of time series in this dataset, it is tough to make any inferences with this visualization.  Let's look @ a different visualization technique..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(ncols=5,nrows=2,figsize=(25,15))\n",
    "axes=axes.reshape(-1)\n",
    "matrix=rel_data[2]\n",
    "i=0\n",
    "for yr in coverage:\n",
    "    Sigma=matrix.xs(yr+'-12-31')\n",
    "    Tops=filter_decile(Sigma,0.8)\n",
    "    G=graphx.Graph()\n",
    "    G.add_nodes_from(Tops.index)\n",
    "    colors=[]\n",
    "    a=axes[i]\n",
    "    a.set_title(yr)\n",
    "    for tick_a in G.nodes:\n",
    "        for tick_b in G.nodes:\n",
    "            p=Sigma[tick_a][tick_b]\n",
    "            if((tick_a!=tick_b) and abs(p)>0.25):\n",
    "                clr='green' if p>0 else 'red'\n",
    "                colors.append(clr)\n",
    "                G.add_edge(tick_a,tick_b,color=clr,weight=abs(p))\n",
    "            else:\n",
    "                pass\n",
    "                #print(tick_a,tick_b)\n",
    "    graphx.draw(G,with_labels=True,edge_color=colors,node_size=20,ax=a)\n",
    "    i+=1\n",
    "fig.savefig(image_path+'Annual Universe Structure.pdf',\n",
    "            orientation='landscape',\n",
    "            pad_inches=0.0,\n",
    "            bbox_inches='tight',\n",
    "            format='pdf'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick in Filed.index:\n",
    "    visualize_stock(tick);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in ['2016','2017','2018','2019','2020']:\n",
    "    entire_year(yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_group(sic_7372,'Group 7372')\n",
    "visualize_group(sic_ex7,'Group ex7372')\n",
    "visualize_group(sic_4,'Group 4')\n",
    "visualize_group(sic_3,'Group 3')\n",
    "visualize_group(sic_56,'Groups 5+6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some simple clustering algorithms on the 2020 subset.  This will provide some insight into the different options we have for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some feature engineers with different Vectorizor parameters (all TfIDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=collect_texts_year('2020')\n",
    "parms={'max_df':0.99,'max_features':10,'min_df':0.01}\n",
    "engineers={}\n",
    "for n in [10,26,50,100,200,500,1000,1500,2000,5000]:\n",
    "    parms={'max_df':0.99,'max_features':n,'min_df':0.01}\n",
    "    engineers[str(n)]=create_extractor(corpus.values(),parms,'max_features'+str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some k-mean clustering modelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg=AgglomerativeClustering(n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings=pd.DataFrame(index=corpus.keys(),columns=engineers.keys())\n",
    "for max_ftr,engineer in engineers.items():\n",
    "    X=engineer.transform(corpus.values())\n",
    "    X_vect=pd.DataFrame.sparse.from_spmatrix(X)\n",
    "    fig,ax=plt.subplots(figsize=(25,10));\n",
    "    ax.set_title(max_ftr);\n",
    "    dendrogram(\n",
    "            linkage(X_vect),\n",
    "            leaf_rotation=45.,\n",
    "            leaf_font_size=20.,\n",
    "            labels=list(corpus.keys()),\n",
    "            p=25,\n",
    "            truncate_mode='level'\n",
    "            );\n",
    "    plt.savefig(image_path+'Dendograms/2020_MaxF'+max_ftr+'.pdf',\n",
    "                orientation='landscape',\n",
    "                pad_inches=0.0,\n",
    "                bbox_inches='tight',\n",
    "                format='pdf'\n",
    "               );\n",
    "    \n",
    "    \n",
    "    Agg.fit(X.toarray())\n",
    "    groupings[max_ftr]=Agg.labels_    \n",
    "groupings.to_excel(parent_path+'Data/Results/Different_Max_Features.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions=pd.DataFrame(index=range(0,20),columns=engineers.keys())\n",
    "for max_feature in engineers.keys():\n",
    "    distributions[max_feature]=groupings[max_feature].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature,engineer in engineers.items():\n",
    "    X=engineer.transform(corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K3=KMeans(n_clusters=3)\n",
    "K5=KMeans(n_clusters=5)\n",
    "K8=KMeans(n_clusters=8)\n",
    "K10=KMeans(n_clusters=10)\n",
    "K15=KMeans(n_clusters=15)\n",
    "K25=KMeans(n_clusters=25)\n",
    "clusterers={'3':K3,'5':K5,'8':K8,'10':K10,'15':K15,'25':K25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings=pd.DataFrame(index=corpus.keys(),columns=engineers.keys())\n",
    "engineer=create_extractor(corpus.values(),parms)\n",
    "X=engineer.transform(corpus.values())\n",
    "for n,clusterer in clusterers.items():\n",
    "    clusterer.fit(X.toarray())\n",
    "    groupings[n]=clusterer.labels_\n",
    "groupings.to_excel(parent_path+'Data/Results/KNNs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aff=AffinityPropagation(max_iter=500,random_state=STATE)\n",
    "parms={'max_df':0.99,'max_features':1000,'min_df':0.01}\n",
    "engineer=create_extractor(corpus.values(),parms)\n",
    "X=engineer.transform(corpus.values())\n",
    "Aff.fit(X.toarray())\n",
    "Aff_Clusters=pd.DataFrame(zip(corpus.keys(),Aff.labels_),columns=['Ticker','Category'])\n",
    "Aff_Clusters.to_excel(parent_path+'Data/Results/Affinity_Labels.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers=Aff.cluster_centers_indices_.tolist()\n",
    "Aff_Summary=Aff_Clusters.loc[centers]\n",
    "Aff_Summary.set_index(['Category'],inplace=True)\n",
    "Aff_Summary['Count']=Aff_Clusters.Category.value_counts()\n",
    "Aff_Summary.to_excel(parent_path+'Data/Results/Affinity_Summary.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full=collect_texts_all()\n",
    "corpus_3yr=collect_texts_years(['2020','2019','2018'])\n",
    "corpus_20=collect_texts_year('2020')\n",
    "corpus_19=collect_texts_year('2019')\n",
    "corpus_18=collect_texts_year('2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parms={'max_df':0.99,'max_features':1000,'min_df':0.01}\n",
    "engineer1=create_extractor(corpus_full.values(),parms,'Alltime')\n",
    "engineer2=create_extractor(corpus_full.values(),parms,'3_Years')\n",
    "engineer3=create_extractor(corpus_20.values(),parms,'2020s')\n",
    "engineer4=create_extractor(corpus_19.values(),parms,'2019s')\n",
    "engineer5=create_extractor(corpus_18.values(),parms,'2018s')\n",
    "\n",
    "X1=engineer1.transform(corpus_20.values())\n",
    "X2=engineer2.transform(corpus_20.values())\n",
    "X3=engineer3.transform(corpus_20.values())\n",
    "X4=engineer4.transform(corpus_20.values())\n",
    "X5=engineer5.transform(corpus_20.values())\n",
    "\n",
    "clusterer=clusterers['15']\n",
    "\n",
    "clusterer.fit(X1.toarray())\n",
    "clustering1=clusterer.labels_\n",
    "\n",
    "clusterer.fit(X2.toarray())\n",
    "clustering2=clusterer.labels_\n",
    "\n",
    "clusterer.fit(X3.toarray())\n",
    "clustering3=clusterer.labels_\n",
    "\n",
    "clusterer.fit(X4.toarray())\n",
    "clustering4=clusterer.labels_\n",
    "\n",
    "clusterer.fit(X5.toarray())\n",
    "clustering5=clusterer.labels_\n",
    "DF=pd.DataFrame(zip(corpus.keys(),clustering1,clustering2,clustering3,clustering4),\n",
    "                columns=['Ticker','Category-Full','Category-3yr','Category-20','Category-19','Category-18'],\n",
    "               )\n",
    "DF.to_excel(parent_path+'Data/Results/Comp_Dictionary.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering w/ Agg.Prop...Done two ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_years=['2020','2019','2018','2017','2016']\n",
    "stocks=Filed[\n",
    "            Filed['2020'] & \n",
    "            Filed['2019'] & \n",
    "            Filed['2018'] & \n",
    "            Filed['2017'] & \n",
    "            Filed['2016']\n",
    "            ].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way 1: Cluster entire corpus simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus=collect_texts_years(five_years)\n",
    "corpus=collect_texts(stocks,five_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms={'max_df':0.99,'max_features':1000,'min_df':0.01}\n",
    "engineer=create_extractor(full_corpus.values(),parms)\n",
    "clusterer=AgglomerativeClustering(n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=engineer.transform(corpus.values())\n",
    "clusterer.fit(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_All=pd.DataFrame(zip(clusterer.labels_,corpus.keys()),columns=['Group','Stock_Year'])\n",
    "DF_All['Stock']=DF_All.Stock_Year.apply(lambda s:s.split('_')[0])\n",
    "DF_All['Year']=DF_All.Stock_Year.apply(lambda s:s.split('_')[1])\n",
    "DF_All.set_index(['Stock','Year'])\n",
    "DF_All.to_excel(parent_path+'Data/Results/5Yr_Simultaneous.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way 2:  Split corpus into 5 parts (one for each year) prior to apply clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_byYear=pd.DataFrame(columns=five_years,index=stocks)\n",
    "parms={'max_df':0.99,'max_features':1000,'min_df':0.01}\n",
    "engineer=create_extractor(full_corpus.values(),parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in five_years:\n",
    "    corpus=collect_texts(stocks,[y])\n",
    "    X=engineer.transform(corpus.values())\n",
    "    clusterer.fit(X.toarray())\n",
    "    DF_byYear[y]=clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_byYear.to_excel(parent_path+'Data/Results/5Yr_Annuals.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Weightings to generate similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={'2020':0.3,'2019':0.2,'2018':0.2,'2017':0.15,'2016':0.15}\n",
    "S=groups_to_similarity(DF_byYear,weights)\n",
    "S.to_excel(parent_path+'Data/Results/5yr_Similarity.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Methodology Except w/ 10 Year horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_years=['2020','2019','2018','2017','2016',\n",
    "            '2015','2014','2013','2012','2011']\n",
    "stocks=Filed[\n",
    "            Filed['2020'] & \n",
    "            Filed['2019'] & \n",
    "            Filed['2018'] & \n",
    "            Filed['2017'] & \n",
    "            Filed['2016'] &\n",
    "            Filed['2015'] & \n",
    "            Filed['2014'] & \n",
    "            Filed['2013'] & \n",
    "            Filed['2012'] & \n",
    "            Filed['2011']\n",
    "            ].index\n",
    "full_corpus=collect_texts_years(ten_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms={'max_df':0.99,'max_features':1000,'min_df':0.01}\n",
    "engineer=create_extractor(full_corpus.values(),parms)\n",
    "clusterer=AgglomerativeClustering(n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_byYear=pd.DataFrame(columns=ten_years,index=stocks)\n",
    "for y in ten_years:\n",
    "    corpus=collect_texts(stocks,[y])\n",
    "    X=engineer.transform(corpus.values())\n",
    "    clusterer.fit(X.toarray())\n",
    "    DF_byYear[y]=clusterer.labels_\n",
    "DF_byYear.to_excel(parent_path+'Data/Results/10Yr_Annuals.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={'2020':0.20,'2019':0.20,'2018':0.15,'2017':0.10,'2016':0.10,\n",
    "         '2015':0.05,'2014':0.05,'2013':0.05,'2012':0.05,'2011':0.05}\n",
    "S=groups_to_similarity(DF_byYear,weights)\n",
    "S.to_excel(parent_path+'Data/Results/10yr_Similarity.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
